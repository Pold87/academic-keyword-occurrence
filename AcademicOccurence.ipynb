{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Academic Occurence](https://github.com/Pold87/academic-keyword-occurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By: Volker Strobel, improved by Patrick Hofmann\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, build_opener, HTTPCookieProcessor\n",
    "from urllib.parse import urlencode\n",
    "from http.cookiejar import MozillaCookieJar\n",
    "import re, time, sys, urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_academic_occurence(search_term, start_date, end_date):\n",
    "    '''\n",
    "    Obtains academic occurence for subject between two dates from Google Scholar and prints it into a csv. \n",
    "    Also supports specific inputs like \"Global Outlook Digital Humanities\" (with quotation marks)\n",
    "    IE: get_academic_occurence('/\"Global Outlook Digital Humanities/\"', 2010, 2012)\n",
    "    '''\n",
    "    #########################\n",
    "    def get_num_results(search_term, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Helper method, sends HTTP request and returns response payload.\n",
    "        \"\"\"\n",
    "\n",
    "        # Open website and read html\n",
    "        user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.109 Safari/537.36'\n",
    "        query_params = { 'q' : search_term, 'as_ylo' : start_date, 'as_yhi' : end_date}\n",
    "        url = \"https://scholar.google.com/scholar?as_vis=1&hl=en&as_sdt=1,5&\" + urllib.parse.urlencode(query_params)\n",
    "        opener = build_opener()\n",
    "        request = Request(url=url, headers={'User-Agent': user_agent})\n",
    "        handler = opener.open(request)\n",
    "        html = handler.read() \n",
    "\n",
    "        # Create soup for parsing HTML and extracting the relevant information\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        div_results = soup.find(\"div\", {\"id\": \"gs_ab_md\"}) # find line 'About x results (y sec)\n",
    "\n",
    "        if div_results != None:\n",
    "\n",
    "            res = re.findall(r'(\\d+).?(\\d+)?.?(\\d+)?\\s', div_results.text) # extract number of search results\n",
    "\n",
    "            if res == []:\n",
    "                num_results = '0'\n",
    "                success = True\n",
    "            else:\n",
    "                num_results = ''.join(res[0]) # convert string to numbe\n",
    "                success = True\n",
    "\n",
    "        else:\n",
    "            success = False\n",
    "            num_results = 0\n",
    "\n",
    "        return num_results, success\n",
    "    ###################################\n",
    "    \n",
    "    values = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 \")\n",
    "    def remover(my_string = \"\"):\n",
    "        for item in my_string:\n",
    "            if item not in values:\n",
    "                my_string = my_string.replace(item, \"\")\n",
    "            return my_string\n",
    "\n",
    "    #####################################\n",
    "    fp = open('occurence_for_{}_{}_{}.csv'.format(remover(search_term),\n",
    "                                                  start_date,\n",
    "                                                  end_date), 'w')\n",
    "\n",
    "    fp.write(\"year,results\\n\")\n",
    "    print(\"year,results\")\n",
    "\n",
    "    for date in range(start_date, end_date + 1):\n",
    "\n",
    "        num_results, success = get_num_results(search_term, date, date)\n",
    "        if not(success):\n",
    "            print(\"It seems that you made to many requests to Google Scholar. Please wait a couple of hours and try again.\")\n",
    "            break\n",
    "        year_results = \"{0},{1}\".format(date, num_results)\n",
    "        print(year_results)\n",
    "        fp.write(year_results + '\\n')\n",
    "        time.sleep(0.8)\n",
    "\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year,results\n",
      "2010,0\n",
      "2011,1\n",
      "2012,1\n",
      "2013,6\n"
     ]
    }
   ],
   "source": [
    "get_academic_occurence('\"Global Outlook Digital Humanities\"', 2010, 2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
